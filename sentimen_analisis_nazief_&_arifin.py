# -*- coding: utf-8 -*-
"""sentimen analisis nazief & arifin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ZHLPN1L37rg6WEjhfChJEA6lI3QxDH1

##crawling
"""

# Import required Python package
!pip install pandas

# Install Node.js (because tweet-harvest built using Node.js)
!sudo apt-get update
!sudo apt-get install -y ca-certificates curl gnupg
!sudo mkdir -p /etc/apt/keyrings
!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg

!NODE_MAJOR=20 && echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list

!sudo apt-get update
!sudo apt-get install nodejs -y

!node -v

# Crawl Data

filename = 'crawll.csv'
search_keyword = 'tiktok shop -filter:retweets lang:id'
limit = 1000

!npx --yes tweet-harvest@latest -o "{filename}" -s "{search_keyword}" -l {limit} --token ""

df = pd.read_csv('/content/tweets-data/crawll.csv', delimiter=";")
df.head(10)

df.drop(df.columns[[0,1,3,4,5,6,7,8,9,10,11]], axis = 1, inplace = True)
df

df.to_csv('tweets.csv', encoding='utf8', index=False)

"""## library"""

import pandas as pd
import tweepy
import re
import csv
import numpy as np

import nltk
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.stem import LancasterStemmer,WordNetLemmatizer
import re, string, unicodedata
from string import punctuation
import nltk
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
nltk.download('stopwords')

"""#Nazief & Adriani

## Pre-proscessing
"""

tweets = pd.read_csv('/content/tweets3.csv')
tweets

def clean_text(text):
    # Menghapus string 'RT' yang biasanya digunakan untuk retweet
    text = re.sub(r'RT[\s]+', '', text)
    # Menghapus teks yang diapit oleh kurung siku, biasanya digunakan untuk menyisipkan informasi tambahan seperti hashtag
    text = re.sub('\[[^]]*\]','',text)
    # Menghapus username, URL, dan karakter non-alfanumerik lainnya menggunakan ekspresi reguler
    text = ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",text).split())
    # Mengganti karakter non-ASCII dengan karakter yang sesuai atau '?' jika karakter tidak bisa diubah
    text = text.encode('ascii', 'replace').decode('ascii')
    # Menghapus tanda pagar '#' yang biasanya digunakan untuk menandai topik atau hashtag
    text = re.sub(r'#', '', text)
    # Menghapus kata tunggal yang berupa huruf, karena tidak memiliki makna yang signifikan
    text = re.sub(r"\b[a-zA-Z]\b", "", text)
    # Menghapus angka dari teks
    text = re.sub(r"\d+", "", text)
    # Menghapus tanda baca menggunakan metode translate()
    text = text.translate(str.maketrans("","",string.punctuation))
    # Menghapus spasi berlebihan
    text = re.sub('\s+',' ',text)
    # Menghapus spasi di awal dan akhir teks
    text = text.strip()
    # Mengubah teks menjadi huruf kecil agar konsisten dalam kasus
    text = text.lower()

    return text

# Membersihkan teks dari kolom 'full_text' dan menyimpannya ke dalam kolom baru 'data_clean'
tweets["data_clean"] = tweets["full_text"].apply(clean_text)

tweets.sort_values("data_clean", inplace = True)
tweets

tweets = tweets.drop_duplicates(subset=['data_clean'])
tweets

#import data normalisasi
normalisasi = pd.read_csv('https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv', index_col='_id')
normalisasi.head()

# Fungsi untuk normalisasi teks
def text_normalize(text):
    # Memisahkan teks menjadi token dan melakukan normalisasi menggunakan data normalisasi yang diberikan
    text = ' '.join([normalisasi[normalisasi['singkat'] == word]['hasil'].values[0] if (normalisasi['singkat'] == word).any() else word for word in text.split()])
    # Mengonversi teks menjadi huruf kecil
    text = str.lower(text)
    return text

# Mengambil sampel data mentah dari kolom 'data_clean'
raw_sample = tweets['data_clean'].iloc[10]
# Melakukan normalisasi pada sampel data mentah
textnorma = text_normalize(raw_sample)

# Menampilkan hasil sebelum dan setelah normalisasi
print('Raw data\t: ', raw_sample)
print('Text Normalize\t: ', textnorma)

tweets['normalisasi'] = tweets['data_clean'].apply(text_normalize)
tweets

def tokenization(text):
    # Memisahkan teks menjadi token menggunakan ekspresi reguler '\W+' yang mencocokkan karakter non-alfanumerik sebagai pemisah
    text = re.split('\W+', text)
    return text

# Melakukan tokenisasi pada teks yang telah dibersihkan ('data_clean') dan menyimpan hasilnya di kolom baru 'tokenisasi'
tweets['tokenisasi'] = tweets['normalisasi'].apply(tokenization)
tweets

text_data = tweets['normalisasi'].values.tolist()
text_data

pip install Sastrawi

# Mengimpor library nltk untuk mengakses kamus stopwords
from nltk.corpus import stopwords

# Mengambil daftar stopwords dalam bahasa Indonesia dari NLTK corpus
stopword = nltk.corpus.stopwords.words('indonesian')

# Mengimpor StopWordRemoverFactory dari Sastrawi untuk mengakses stopwords tambahan
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# Mengakses kamus stopwords tambahan dari Sastrawi dan menambahkannya ke dalam set stopwords
stop_factory = StopWordRemoverFactory().get_stop_words()
stopword = set(stopword)

# Fungsi untuk menghapus stopwords dari teks
def stopwords(text):
    # Memfilter kata-kata dalam teks dan hanya menyertakan kata-kata yang bukan stopwords
    text = [word for word in text if word not in stopword]
    return text

# Menggunakan fungsi stopwords pada kolom 'tokenisasi' dari DataFrame tweets
tweets['stopwords'] = tweets['tokenisasi'].apply(stopwords)
# Menampilkan lima baris pertama dari DataFrame tweets setelah menghapus stopwords
tweets

import time
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.corpus import stopwords
# Membuat objek factory untuk membuat stemmer
factory = StemmerFactory()
# Membuat stemmer menggunakan factory
stemmer = factory.create_stemmer()

# Fungsi untuk melakukan stemming pada suatu term
def stemmed_wrapper(term):
    return stemmer.stem(term)

# Membuat kamus kosong untuk menyimpan term yang telah di-stem
term_dict = {}
# Variabel hitung digunakan untuk melacak jumlah term yang di-stem
hitung = 0

# Mulai pengukuran waktu untuk membangun kamus term
start_time_dict_building = time.time()

# Melakukan iterasi pada setiap dokumen dalam kolom 'stopwords' dari DataFrame tweets
for document in tweets['stopwords']:
    # Melakukan iterasi pada setiap term dalam dokumen
    for term in document:
        # Jika term belum ada dalam kamus, tambahkan term ke kamus dengan nilai kosong
        if term not in term_dict:
            term_dict[term] = ' '

# Menghitung waktu yang dibutuhkan untuk membangun kamus term
end_time_dict_building = time.time()
time_dict_building = end_time_dict_building - start_time_dict_building

print("Waktu proses membangun kamus term: {:.4f} detik".format(time_dict_building))
print(len(term_dict))
print("------------------------")

# Mulai pengukuran waktu untuk proses stemming pada setiap term dalam kamus
start_time_stemming = time.time()

# Melakukan stemming pada setiap term dalam kamus
for term in term_dict:
    term_start_time = time.time()
    term_dict[term] = stemmed_wrapper(term)
    term_end_time = time.time()
    term_stemming_time = term_end_time - term_start_time
    # Menghitung jumlah term yang sudah di-stem
    hitung += 1
    # Menampilkan hasil stemming term per term dengan waktu prosesnya
    print(hitung, ":", term, ":", term_dict[term], ":", "{:.4f} detik".format(term_stemming_time))

# Menghitung waktu yang dibutuhkan untuk proses stemming
end_time_stemming = time.time()
time_stemming = end_time_stemming - start_time_stemming

print("Waktu total proses stemming: {:.4f} detik".format(time_stemming))
print(term_dict)
print("------------------------")

# Fungsi untuk menerapkan stemming pada setiap term dalam suatu dokumen
def get_stemmed_term(document):
    return [term_dict[term] for term in document]

# Mulai pengukuran waktu untuk menerapkan stemming pada setiap dokumen
start_time_apply_stemming = time.time()

# Menggunakan fungsi get_stemmed_term pada kolom 'stopwords' dari DataFrame tweets
tweets['stemming'] = tweets['stopwords'].apply(get_stemmed_term)

# Menampilkan 20 baris pertama DataFrame tweets setelah proses stemming
tweets.head(20)

# Definisi fungsi fit_stopwords dengan parameter text
def fit_stopwords(text):
    # Mengonversi text menjadi numpy array
    text = np.array(text)
    # Menggabungkan semua elemen dalam numpy array menjadi satu string
    text = ' '.join(text)
    # Mengembalikan text yang telah digabungkan
    return text

# Menggunakan fungsi fit_stopwords pada kolom 'steaming' dari DataFrame tweets
# dan hasilnya disimpan dalam kolom 'teks'
tweets['teks'] = tweets['stemming'].apply(lambda x: fit_stopwords(x))
tweets

tweets.drop(tweets.columns[[0,1,2,3,4,5]], axis = 1, inplace = True)
tweets.head()

tweets.to_csv('prepocessing_nazief.csv', encoding='utf8', index=False)

"""##translate"""

!pip3 install googletrans==3.1.0a0

import pandas as pd
import googletrans
from googletrans import Translator

data = pd.read_csv('/content/prepocessing_nazief.csv')
data.head(10)

translator = Translator()
translations = {}
for column in data.columns:
  # Unique elements of the column
  unique_elements = data[column].unique()
  for element in unique_elements:
    # Adding all the translations to a dictionary (translations)
    translations[element] = translator.translate(element).text
translations

data.replace(translations, inplace = True)
data.head(10)

data = data['teks'].str.lower()

data

data.to_csv('translate_nazief.csv', encoding='utf8', index=False)

"""##labelling"""

df2 = pd.read_csv('/content/translate_nazief.csv')
df2.head(10)

#LEXICON BASED
!pip install VaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()

scores = [analyser.polarity_scores(x) for x in df2['teks']]
print(scores)
df2['Compound_Score'] = [x['compound'] for x in scores]

df2

df2.nsmallest(10, ['Compound_Score'])

#compound score lexicon based
df2.loc[df2['Compound_Score'] < 0, 'Sentiments'] = 'Negatif'

df2.loc[df2['Compound_Score'] == 0, 'Sentiments'] = 'Netral'

df2.loc[df2['Compound_Score'] > 0, 'Sentiments'] = 'Positif'
df2

df2.to_csv('Hasillabelling_nazief.csv', encoding='utf8', index=False)

data = pd.read_csv('/content/Hasillabelling_nazief.csv')
data.head(10)

data.info()

# Menghitung jumlah data untuk setiap label sentimen
sentiment_counts = data['Sentiments'].value_counts()

# Menampilkan hasil
print("Jumlah data untuk setiap sentimen:")
print(f"Negatif: {sentiment_counts.get('Negatif', 0)}")
print(f"Netral: {sentiment_counts.get('Netral', 0)}")
print(f"Positif: {sentiment_counts.get('Positif', 0)}")

s = pd.value_counts(data['Sentiments'])
ax = s.plot.bar()
n = len(data.index)
print(n)
for p in ax.patches:
  ax.annotate(str(round(p.get_height() / n*100, 2)) + '%',(p.get_x()*1.005, p.get_height()*1.005))

"""## Modeling

### SVM
"""

df3 = pd.read_csv('/content/Hasillabelling_nazief.csv')
df3.head()

from sklearn.model_selection import train_test_split

y = df3.Sentiments.values
x = df3.teks.values

x_train, x_test, y_train, y_test = train_test_split(x, y, stratify= y, random_state=1, test_size=0.2, shuffle=True)

print(x_train.shape)
print(x_test.shape)

from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

from imblearn.over_sampling import RandomOverSampler

from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi objek TfidfVectorizer dengan konfigurasi tertentu
vectorizer = TfidfVectorizer(stop_words='english')

# Melakukan fitting dan transformasi pada data latih dan data uji
x_train_vec = vectorizer.fit_transform(x_train)
x_test_vec = vectorizer.transform(x_test)

# Menampilkan dimensi matriks vektorisasi
print(x_train_vec.shape)
print(x_test_vec.shape)

print(x_train_vec)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

"""#### kernel linier"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report
from sklearn.svm import SVC

# Inisialisasi model SVM dengan kernel linear
svm_linear = SVC(kernel='linear', probability=True, C=1)

# Latih model SVM dengan kernel linear pada data latih
svm_linear.fit(x_train_vec, y_train)

# Memprediksi label kelas untuk data uji
y_pred_svm_linear = svm_linear.predict(x_test_vec)

# Evaluasi kinerja model
accuracy = accuracy_score(y_test, y_pred_svm_linear)
conf_matrix = confusion_matrix(y_test, y_pred_svm_linear)
precision = precision_score(y_test, y_pred_svm_linear, average='macro')
recall = recall_score(y_test, y_pred_svm_linear, average='macro')

# Menampilkan hasil evaluasi
print("Accuracy Linear: ", accuracy * 100, '%')
print("Confusion Matrix Linear:")
print(conf_matrix)
print("Precision Linear: ", precision)
print("Recall Score : ", recall)

# Classification report
print("Classification Report:")
print(classification_report(y_test, y_pred_svm_linear))

"""### naive bayes

#### ComplementNB
"""

from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Inisialisasi model Complement Naive Bayes
CNB = ComplementNB()

# Melatih model menggunakan data pelatihan yang sudah di-vektorisasi
CNB.fit(x_train_vec, y_train)

# Melakukan prediksi pada data uji yang sudah di-vektorisasi
predicted = CNB.predict(x_test_vec)

# Evaluasi kinerja model
accuracy = accuracy_score(y_test, predicted)
conf_matrix = confusion_matrix(y_test, predicted)
precision = precision_score(y_test, predicted, average='macro')
recall = recall_score(y_test, predicted, average='macro')

# Menampilkan hasil evaluasi
print("Accuracy CNB: ", accuracy * 100, '%')
print("Confusion Matrix CNB:")
print(conf_matrix)
print("Precision CNB: ", precision)
print("Recall Score : ", recall)

# Classification report
print('Classification Report:')
print(classification_report(y_test, predicted))

"""#Arifin Setiono

## Pre-proscessing
"""

tweets = pd.read_csv('/content/tweets3.csv')
tweets.head(10)

def clean_text(text):
    # Menghapus string 'RT' yang biasanya digunakan untuk retweet
    text = re.sub(r'RT[\s]+', '', text)
    # Menghapus teks yang diapit oleh kurung siku, biasanya digunakan untuk menyisipkan informasi tambahan seperti hashtag
    text = re.sub('\[[^]]*\]','',text)
    # Menghapus username, URL, dan karakter non-alfanumerik lainnya menggunakan ekspresi reguler
    text = ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",text).split())
    # Mengganti karakter non-ASCII dengan karakter yang sesuai atau '?' jika karakter tidak bisa diubah
    text = text.encode('ascii', 'replace').decode('ascii')
    # Menghapus tanda pagar '#' yang biasanya digunakan untuk menandai topik atau hashtag
    text = re.sub(r'#', '', text)
    # Menghapus kata tunggal yang berupa huruf, karena tidak memiliki makna yang signifikan
    text = re.sub(r"\b[a-zA-Z]\b", "", text)
    # Menghapus angka dari teks
    text = re.sub(r"\d+", "", text)
    # Menghapus tanda baca menggunakan metode translate()
    text = text.translate(str.maketrans("","",string.punctuation))
    # Menghapus spasi berlebihan
    text = re.sub('\s+',' ',text)
    # Menghapus spasi di awal dan akhir teks
    text = text.strip()
    # Mengubah teks menjadi huruf kecil agar konsisten dalam kasus
    text = text.lower()

    return text

# Membersihkan teks dari kolom 'full_text' dan menyimpannya ke dalam kolom baru 'data_clean'
tweets["data_clean"] = tweets["full_text"].apply(clean_text)

tweets.sort_values("data_clean", inplace = True)
tweets

tweets = tweets.drop_duplicates(subset=['data_clean'])
tweets

#import data normalisasi
normalisasi = pd.read_csv('https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv', index_col='_id')
normalisasi.head()

# Fungsi untuk normalisasi teks
def text_normalize(text):
    # Memisahkan teks menjadi token dan melakukan normalisasi menggunakan data normalisasi yang diberikan
    text = ' '.join([normalisasi[normalisasi['singkat'] == word]['hasil'].values[0] if (normalisasi['singkat'] == word).any() else word for word in text.split()])
    # Mengonversi teks menjadi huruf kecil
    text = str.lower(text)
    return text

# Mengambil sampel data mentah dari kolom 'data_clean'
raw_sample = tweets['data_clean'].iloc[10]
# Melakukan normalisasi pada sampel data mentah
textnorma = text_normalize(raw_sample)

# Menampilkan hasil sebelum dan setelah normalisasi
print('Raw data\t: ', raw_sample)
print('Text Normalize\t: ', textnorma)

tweets['normalisasi'] = tweets['data_clean'].apply(text_normalize)
tweets

def tokenization(text):
    # Memisahkan teks menjadi token menggunakan ekspresi reguler '\W+' yang mencocokkan karakter non-alfanumerik sebagai pemisah
    text = re.split('\W+', text)
    return text

# Melakukan tokenisasi pada teks yang telah dibersihkan ('data_clean') dan menyimpan hasilnya di kolom baru 'tokenisasi'
tweets['tokenisasi'] = tweets['normalisasi'].apply(tokenization)
tweets

text_data = tweets['normalisasi'].values.tolist()
text_data

pip install Sastrawi

# Mengimpor library nltk untuk mengakses kamus stopwords
from nltk.corpus import stopwords

# Mengambil daftar stopwords dalam bahasa Indonesia dari NLTK corpus
stopword = nltk.corpus.stopwords.words('indonesian')

# Mengimpor StopWordRemoverFactory dari Sastrawi untuk mengakses stopwords tambahan
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# Mengakses kamus stopwords tambahan dari Sastrawi dan menambahkannya ke dalam set stopwords
stop_factory = StopWordRemoverFactory().get_stop_words()
stopword = set(stopword)

# Fungsi untuk menghapus stopwords dari teks
def stopwords(text):
    # Memfilter kata-kata dalam teks dan hanya menyertakan kata-kata yang bukan stopwords
    text = [word for word in text if word not in stopword]
    return text

# Menggunakan fungsi stopwords pada kolom 'tokenisasi' dari DataFrame tweets
tweets['stopwords'] = tweets['tokenisasi'].apply(stopwords)
# Menampilkan lima baris pertama dari DataFrame tweets setelah menghapus stopwords
tweets.head()

kamus_df = pd.read_csv('/content/kamus-kata-dasar.csv')
kamus_df.head(5)

kamus = kamus_df['a'].tolist()

kamus_df.info()

import time

# Definisikan fungsi cek_kamus dan algoritma stemming Arifin seperti yang ada di kode awal
def cek_kamus(kata, kamus_set):
    return kata in kamus_set

kamus_set = set(kamus)

def potong_awalan_me(kata):
    _2huruf_awal = kata[:2]
    _4huruf_awal = kata[:4]
    _3huruf_awal = kata[:3]
    awalan = ""

    if _4huruf_awal in ["meng", "peng", "meny", "peny"]:
        awalan = _4huruf_awal
    elif _3huruf_awal in ["mem", "pem", "men", "pen", "per"]:
        awalan = _3huruf_awal
    elif _2huruf_awal in ["me", "pe"]:
        awalan = _2huruf_awal

    return awalan

def potong_awalan_be(kata):
    awalan = ""
    _3huruf_awal = kata[:3]

    if _3huruf_awal == "ber" or (_3huruf_awal == "bel" and kata == "belajar"):
        awalan = _3huruf_awal
    elif kata[:2] == "be" and kata == "bekerja":
        awalan = kata[:2]

    return awalan

def potong_awalan_lainnya(kata):
    awalan = ""
    awalan_lain = ["di", "ke", "ku", "se"]
    _2huruf_awal = kata[:2]

    if kata[:3] == "ter":
        awalan = kata[:3]
    elif _2huruf_awal in awalan_lain:
        awalan = _2huruf_awal

    return awalan

def potong_awalan(kata):
    kata = kata.lower()
    _2huruf_awal = kata[:2]
    awalan = ["", ""]

    for i in range(2):
        if _2huruf_awal == "me" or _2huruf_awal == "pe":
            awalan_tmp = potong_awalan_me(kata)
        elif _2huruf_awal == "be":
            awalan_tmp = potong_awalan_be(kata)
        else:
            awalan_tmp = potong_awalan_lainnya(kata)

        if awalan_tmp:
            pjg_awalan = len(awalan_tmp)
            kata = kata[pjg_awalan:]
            _2huruf_awal = kata[:2]

            if awalan_tmp in ["ber", "ter", "per"]:
                awalan[1] = awalan_tmp
            else:
                if not awalan[0]:
                    awalan[0] = awalan_tmp
                else:
                    awalan[1] = awalan_tmp

    return awalan

def potong_akhiran(kata):
    kata = kata.lower()
    akhiran1 = ['lah', 'kah', 'pun', 'tah']
    akhiran2 = ['ku', 'mu', 'nya']
    akhiran3 = ['i', 'an', 'kan']
    akhir = ["", "", ""]

    for i in range(3):
        _3huruf_akhir = kata[-3:]
        _2huruf_akhir = kata[-2:]
        _1huruf_akhir = kata[-1:]

        if i == 0 and _3huruf_akhir in akhiran1:
            akhir[i] = _3huruf_akhir
            kata = kata[:-3]
        elif i == 1:
            if _3huruf_akhir in akhiran2:
                akhir[i] = _3huruf_akhir
                kata = kata[:-3]
            elif _2huruf_akhir in akhiran2:
                akhir[i] = _2huruf_akhir
                kata = kata[:-2]
        elif i == 2:
            if _3huruf_akhir in akhiran3:
                akhir[i] = _3huruf_akhir
            elif _2huruf_akhir in akhiran3:
                akhir[i] = _2huruf_akhir
            elif _1huruf_akhir in akhiran3:
                akhir[i] = _1huruf_akhir

    return akhir

def cari_kata_dasar(kata):
    awalan = potong_awalan(kata)
    akhiran = potong_akhiran(kata)
    panjang2_awalan = len(awalan[0]) + len(awalan[1])
    panjang3_akhiran = len(akhiran[0]) + len(akhiran[1]) + len(akhiran[2])
    kata_dasar = kata[panjang2_awalan:len(kata) - panjang3_akhiran]

    return kata_dasar

def stemming_arifin(kata, kamus_set):
    if cek_kamus(kata, kamus_set):
        return kata

    awalan = potong_awalan(kata)
    akhiran = potong_akhiran(kata)
    kata_dasar = cari_kata_dasar(kata)

    potensi_kata = [
        "k" + kata_dasar if awalan[0][2:4] == "ng" else "",
        "s" + kata_dasar if awalan[0][2:4] == "ny" else "",
        "p" + kata_dasar if awalan[0][2:3] == "m" else "",
        "t" + kata_dasar if awalan[0][2:3] == "n" else "",
        awalan[1] + kata_dasar + akhiran[2] + akhiran[1] + akhiran[0],
        kata_dasar + akhiran[2] + akhiran[1] + akhiran[0],
        kata_dasar + akhiran[2] + akhiran[1],
        kata_dasar + akhiran[2],
        kata_dasar,
        awalan[0] + kata_dasar,
        awalan[0] + awalan[1] + kata_dasar,
        awalan[0] + awalan[1] + kata_dasar + akhiran[2],
        awalan[0] + awalan[1] + kata_dasar + akhiran[2] + akhiran[1],
        awalan[1] + kata_dasar,
        awalan[1] + kata_dasar + akhiran[2],
        awalan[1] + kata_dasar + akhiran[2] + akhiran[1]
    ]

    for p_kata in potensi_kata:
        if p_kata and cek_kamus(p_kata, kamus_set):
            return p_kata

    return kata

# Menambahkan waktu proses
start_time = time.time()

# Membuat kamus kosong untuk menyimpan term yang telah di-stem
term_dict_arifin = {}
# Variabel hitung digunakan untuk melacak jumlah term yang di-stem
hitung_arifin = 0

# Mulai pengukuran waktu untuk membangun kamus term
start_time_dict_building = time.time()

# Melakukan iterasi pada setiap dokumen dalam kolom 'stopwords' dari DataFrame tweets
for document in tweets['stopwords']:
    # Melakukan iterasi pada setiap term dalam dokumen
    for term in document:
        # Jika term belum ada dalam kamus, tambahkan term ke kamus dengan nilai kosong
        if term not in term_dict_arifin:
            term_dict_arifin[term] = ' '

# Menghitung waktu yang dibutuhkan untuk membangun kamus term
end_time_dict_building = time.time()
time_dict_building = end_time_dict_building - start_time_dict_building

print("Waktu proses membangun kamus term: {:.4f} detik".format(time_dict_building))
print(len(term_dict_arifin))
print("------------------------")

# Mulai pengukuran waktu untuk proses stemming pada setiap term dalam kamus
start_time_stemming = time.time()

# Melakukan stemming pada setiap term dalam kamus
for term in term_dict_arifin:
    term_start_time = time.time()
    term_dict_arifin[term] = stemming_arifin(term, kamus_set)
    term_end_time = time.time()
    term_stemming_time = term_end_time - term_start_time
    # Menghitung jumlah term yang sudah di-stem
    hitung_arifin += 1
    # Menampilkan hasil stemming term per term dengan waktu prosesnya
    print(f"{hitung_arifin} : {term} : {term_dict_arifin[term]} : {term_stemming_time:.4f} detik")

# Menghitung waktu yang dibutuhkan untuk proses stemming
end_time_stemming = time.time()
time_stemming = end_time_stemming - start_time_stemming

print("Waktu total proses stemming: {:.4f} detik".format(time_stemming))
print(term_dict_arifin)
print("------------------------")

# Fungsi untuk menerapkan stemming pada setiap term dalam suatu dokumen
def get_stemmed_term_arifin(document):
    return [term_dict_arifin[term] for term in document]

# Mulai pengukuran waktu untuk menerapkan stemming pada setiap dokumen
start_time_apply_stemming = time.time()

# Menggunakan fungsi get_stemmed_term_arifin pada kolom 'stopwords' dari DataFrame tweets
tweets['stemming_arifin'] = tweets['stopwords'].apply(get_stemmed_term_arifin)

# Menampilkan 20 baris pertama DataFrame tweets setelah proses stemming
tweets

# Definisi fungsi fit_stopwords dengan parameter text
def fit_stopwords(text):
    # Mengonversi text menjadi numpy array
    text = np.array(text)
    # Menggabungkan semua elemen dalam numpy array menjadi satu string
    text = ' '.join(text)
    # Mengembalikan text yang telah digabungkan
    return text

# Menggunakan fungsi fit_stopwords pada kolom 'steaming' dari DataFrame tweets
# dan hasilnya disimpan dalam kolom 'teks'
tweets['teks'] = tweets['stemming'].apply(lambda x: fit_stopwords(x))
tweets

tweets.drop(tweets.columns[[0,1,2,3,4,5]], axis = 1, inplace = True)
tweets.head()

tweets.to_csv('prepocessing_arifin.csv', encoding='utf8', index=False)

"""##translate"""

!pip3 install googletrans==3.1.0a0

import pandas as pd
import googletrans
from googletrans import Translator

data = pd.read_csv('/content/prepocessing_arifin.csv')
data.head(10)

translator = Translator()
translations = {}
for column in data.columns:
  # Unique elements of the column
  unique_elements = data[column].unique()
  for element in unique_elements:
    # Adding all the translations to a dictionary (translations)
    translations[element] = translator.translate(element).text
translations

data.replace(translations, inplace = True)
data.head(10)

data = data['teks'].str.lower()

data

data.to_csv('translate_arifin.csv', encoding='utf8', index=False)

"""##labelling"""

df2 = pd.read_csv('/content/translate_arifin.csv')
df2.head(10)

import nltk
nltk.download('punkt')

import nltk
nltk.download('averaged_perceptron_tagger')

from textblob import TextBlob

for tweet in df2.teks:
  clean_tweet= tweet

  blob_object = TextBlob(clean_tweet)
  hasil = blob_object.tags
  print(hasil)

polarity = lambda x: TextBlob(x).sentiment.polarity

df2['Compound_Score'] = df2['teks'].apply(polarity)
df2

#LEXICON BASED
!pip install VaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()

scores = [analyser.polarity_scores(x) for x in df2['teks']]
print(scores)
df2['Compound_Score'] = [x['compound'] for x in scores]

df2.head()

df2.nsmallest(10, ['Compound_Score'])

#compound score lexicon based
df2.loc[df2['Compound_Score'] < 0, 'Sentiments'] = 'Negatif'

df2.loc[df2['Compound_Score'] == 0, 'Sentiments'] = 'Netral'

df2.loc[df2['Compound_Score'] > 0, 'Sentiments'] = 'Positif'
df2.head(15)

df2.to_csv('Hasillabelling_arifin.csv', encoding='utf8', index=False)

data = pd.read_csv('/content/Hasillabelling_arifin.csv')
data

data.info()

s = pd.value_counts(data['Sentiments'])
ax = s.plot.bar()
n = len(data.index)
print(n)
for p in ax.patches:
  ax.annotate(str(round(p.get_height() / n*100, 2)) + '%',(p.get_x()*1.005, p.get_height()*1.005))

"""## Modeling

### SVM
"""

df3 = pd.read_csv('/content/Hasillabelling_arifin.csv')
df3.head()

from sklearn.model_selection import train_test_split

y = df3.Sentiments.values
x = df3.teks.values

x_train, x_test, y_train, y_test = train_test_split(x, y, stratify= y, random_state=1, test_size=0.2, shuffle=True)

print(x_train.shape)
print(x_test.shape)

from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi objek TfidfVectorizer dengan konfigurasi tertentu
vectorizer = TfidfVectorizer(stop_words='english')

# Melakukan fitting dan transformasi pada data latih dan data uji
x_train_vec = vectorizer.fit_transform(x_train)
x_test_vec = vectorizer.transform(x_test)

# Menampilkan dimensi matriks vektorisasi
print(x_train_vec.shape)
print(x_test_vec.shape)

print(x_train_vec)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

"""#### kernel linier"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report
from sklearn.svm import SVC

# Inisialisasi model SVM dengan kernel linear
svm_linear = SVC(kernel='linear', probability=True, C=1)

# Latih model SVM dengan kernel linear pada data latih
svm_linear.fit(x_train_vec, y_train)

# Memprediksi label kelas untuk data uji
y_pred_svm_linear = svm_linear.predict(x_test_vec)

# Evaluasi kinerja model
accuracy = accuracy_score(y_test, y_pred_svm_linear)
conf_matrix = confusion_matrix(y_test, y_pred_svm_linear)
precision = precision_score(y_test, y_pred_svm_linear, average='macro')
recall = recall_score(y_test, y_pred_svm_linear, average='macro')

# Menampilkan hasil evaluasi
print("Accuracy Linear: ", accuracy * 100, '%')
print("Confusion Matrix Linear:")
print(conf_matrix)
print("Precision Linear: ", precision)
print("Recall Score : ", recall)

# Classification report
print("Classification Report:")
print(classification_report(y_test, y_pred_svm_linear))

"""### naive bayes

#### ComplementNB
"""

from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Inisialisasi model Complement Naive Bayes
CNB = ComplementNB()

# Melatih model menggunakan data pelatihan yang sudah di-vektorisasi
CNB.fit(x_train_vec, y_train)

# Melakukan prediksi pada data uji yang sudah di-vektorisasi
predicted = CNB.predict(x_test_vec)

# Evaluasi kinerja model
accuracy = accuracy_score(y_test, predicted)
conf_matrix = confusion_matrix(y_test, predicted)
precision = precision_score(y_test, predicted, average='macro')
recall = recall_score(y_test, predicted, average='macro')

# Menampilkan hasil evaluasi
print("Accuracy CNB: ", accuracy * 100, '%')
print("Confusion Matrix CNB:")
print(conf_matrix)
print("Precision CNB: ", precision)
print("Recall Score : ", recall)

# Classification report
print('Classification Report:')
print(classification_report(y_test, predicted))